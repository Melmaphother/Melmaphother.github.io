<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#eb77bd" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#db448c" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/project/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/project/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/project/images/favicon-16x16.png">
  <link rel="mask-icon" href="/project/images/logo.svg" color="#eb77bd">

<link rel="stylesheet" href="/project/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"melmaphother.github.io","root":"/project/","images":"/project/images","scheme":"Pisces","darkmode":true,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/project/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/project/js/config.js"></script>

    <meta name="description" content="这篇文章是我关于 Denoising Diffusion Probabilistic Model 文章的解读。">
<meta property="og:type" content="article">
<meta property="og:title" content="Denoising Diffusion Probabilistic Model 解读">
<meta property="og:url" content="https://melmaphother.github.io/project/2024/08/08/Denoising-Diffusion-Probabilistic-Model/index.html">
<meta property="og:site_name" content="Melmaphother">
<meta property="og:description" content="这篇文章是我关于 Denoising Diffusion Probabilistic Model 文章的解读。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://melmaphother.github.io/project/2024/08/08/Denoising-Diffusion-Probabilistic-Model/1.png">
<meta property="og:image" content="https://melmaphother.github.io/project/2024/08/08/Denoising-Diffusion-Probabilistic-Model/2.png">
<meta property="og:image" content="https://melmaphother.github.io/project/2024/08/08/Denoising-Diffusion-Probabilistic-Model/3.png">
<meta property="article:published_time" content="2024-08-08T13:22:09.000Z">
<meta property="article:modified_time" content="2024-08-24T09:32:35.507Z">
<meta property="article:author" content="Daoyu Wang">
<meta property="article:tag" content="Diffusion">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://melmaphother.github.io/project/2024/08/08/Denoising-Diffusion-Probabilistic-Model/1.png">


<link rel="canonical" href="https://melmaphother.github.io/project/2024/08/08/Denoising-Diffusion-Probabilistic-Model/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://melmaphother.github.io/project/2024/08/08/Denoising-Diffusion-Probabilistic-Model/","path":"2024/08/08/Denoising-Diffusion-Probabilistic-Model/","title":"Denoising Diffusion Probabilistic Model 解读"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Denoising Diffusion Probabilistic Model 解读 | Melmaphother</title>
  








  <noscript>
    <link rel="stylesheet" href="/project/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/project/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Melmaphother</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">盈盈一水间，脉脉不得语</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/project/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/project/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/project/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/project/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/project/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#denoising-diffusion-model"><span class="nav-number">1.</span> <span class="nav-text">Denoising Diffusion Model</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80"><span class="nav-number">1.1.</span> <span class="nav-text">数学基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#jenson-%E4%B8%8D%E7%AD%89%E5%BC%8F"><span class="nav-number">1.1.1.</span> <span class="nav-text">Jenson 不等式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%96%E7%AD%89%E6%9D%A1%E4%BB%B6"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">取等条件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kl-%E6%95%A3%E5%BA%A6-kl-divergence"><span class="nav-number">1.1.2.</span> <span class="nav-text">KL 散度 KL Divergence</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-1"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E6%80%A7"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">特性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96%E6%96%B9%E6%B3%95-reparameterization"><span class="nav-number">1.1.3.</span> <span class="nav-text">重参数化方法
Reparameterization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E7%BA%BF%E6%80%A7%E7%BB%84%E5%90%88"><span class="nav-number">1.1.4.</span> <span class="nav-text">高斯分布的线性组合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0-lf-%E5%92%8C%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1-mle"><span class="nav-number">1.1.5.</span> <span class="nav-text">似然函数 LF 和最大似然估计
MLE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#expectation-maximization-%E7%AE%97%E6%B3%95%E5%92%8C%E7%BD%AE%E4%BF%A1%E4%B8%8B%E7%95%8C-elbo"><span class="nav-number">1.1.6.</span> <span class="nav-text">Expectation-Maximization
算法和置信下界 ELBO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%94%E5%90%88%E5%88%86%E5%B8%83%E5%92%8C%E8%BE%B9%E7%BC%98%E5%88%86%E5%B8%83"><span class="nav-number">1.1.7.</span> <span class="nav-text">联合分布和边缘分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fubini-%E5%AE%9A%E7%90%86"><span class="nav-number">1.1.8.</span> <span class="nav-text">Fubini 定理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B-generative-model"><span class="nav-number">1.2.</span> <span class="nav-text">生成模型 Generative Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#variation-autoencoder-vae"><span class="nav-number">1.2.1.</span> <span class="nav-text">Variation AutoEncoder VAE</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#denoising-diffusion-probabilistic-model"><span class="nav-number">1.3.</span> <span class="nav-text">Denoising Diffusion
Probabilistic Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E8%BF%87%E7%A8%8B"><span class="nav-number">1.3.1.</span> <span class="nav-text">前向过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">简单介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E6%8F%8F%E8%BF%B0"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">数学描述</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E8%BF%87%E7%A8%8B"><span class="nav-number">1.3.2.</span> <span class="nav-text">反向过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D-1"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">简单介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E6%8F%8F%E8%BF%B0-1"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">数学描述</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#denoising-diffusion-model-%E7%9A%84%E8%AE%AD%E7%BB%83%E5%92%8C%E9%87%87%E6%A0%B7"><span class="nav-number">1.3.3.</span> <span class="nav-text">Denoising Diffusion
Model 的训练和采样</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">损失函数推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l_t-1-%E9%A1%B9%E8%AE%A8%E8%AE%BA"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">\(L_{t-1}\)
项讨论</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l_0-%E9%A1%B9%E8%AE%A8%E8%AE%BA"><span class="nav-number">1.3.3.3.</span> <span class="nav-text">\(L_0\) 项讨论</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E5%92%8C%E9%87%87%E6%A0%B7"><span class="nav-number">1.3.3.4.</span> <span class="nav-text">训练和采样</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Daoyu Wang"
      src="/project/uploads/My-avatar.jpg">
  <p class="site-author-name" itemprop="name">Daoyu Wang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/project/archives/">
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/project/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/project/tags/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Melmaphother" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Melmaphother" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:melmaphother@gmail.com" title="E-Mail → mailto:melmaphother@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Melmaphother" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;Melmaphother" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/22767428/melmaphother" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;22767428&#x2F;melmaphother" rel="noopener me" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i>StackOverflow</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://melmaphother.github.io/project/2024/08/08/Denoising-Diffusion-Probabilistic-Model/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/project/uploads/My-avatar.jpg">
      <meta itemprop="name" content="Daoyu Wang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Melmaphother">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Denoising Diffusion Probabilistic Model 解读 | Melmaphother">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Denoising Diffusion Probabilistic Model 解读
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-08-08 21:22:09" itemprop="dateCreated datePublished" datetime="2024-08-08T21:22:09+08:00">2024-08-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-08-24 17:32:35" itemprop="dateModified" datetime="2024-08-24T17:32:35+08:00">2024-08-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/project/categories/Diffusion/" itemprop="url" rel="index"><span itemprop="name">Diffusion</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>这篇文章是我关于 Denoising Diffusion Probabilistic Model
文章的解读。</p>
<span id="more"></span>
<h1 id="denoising-diffusion-model">Denoising Diffusion Model</h1>
<h2 id="数学基础">数学基础</h2>
<h3 id="jenson-不等式">Jenson 不等式</h3>
<h4 id="定义">定义</h4>
<p>设 <span class="math inline">\(X\)</span> 是一个随机变量，<span
class="math inline">\(f\)</span> 是一个定义在 <span
class="math inline">\(X\)</span> 的值域上的凸函数。那么，Jensen
不等式表示为：</p>
<p><span class="math display">\[ f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]
\]</span></p>
<p>其中，<span class="math inline">\(\mathbb{E}[X]\)</span> 是 <span
class="math inline">\(X\)</span> 的期望值，<span
class="math inline">\(\mathbb{E}[f(X)]\)</span> 是 <span
class="math inline">\(f(X)\)</span> 的期望值。</p>
<h4 id="取等条件">取等条件</h4>
<p>Jensen 不等式取等号的条件是：</p>
<ol type="1">
<li><span class="math inline">\(f\)</span> 是一个线性函数，即 <span
class="math inline">\(f(x) = ax + b\)</span> 形式的函数，其中 <span
class="math inline">\(a\)</span> 和 <span
class="math inline">\(b\)</span> 是常数。</li>
<li>或者，随机变量 <span class="math inline">\(X\)</span> 是退化的（即
<span class="math inline">\(X\)</span> 是常数），因此 <span
class="math inline">\(\mathbb{E}[X] = X\)</span>。</li>
</ol>
<h3 id="kl-散度-kl-divergence">KL 散度 KL Divergence</h3>
<h4 id="定义-1">定义</h4>
<p>对于两个概率分布 P 和 Q，KL 散度 <span
class="math inline">\(D_{KL}(P||Q)\)</span> 定义如下：</p>
<p><span class="math display">\[
D_{KL}(P || Q) = \sum_{x\in \chi}P(x)\log \frac{P(x)}{Q(x)}
\]</span></p>
<p>连续情况下：</p>
<p><span class="math display">\[
D_{KL}(P || Q) = \int_{-\infty}^{\infty}p(x)\log \frac{p(x)}{q(x)}
\]</span></p>
<p>KL 散度衡量的是当我们使用分布 Q 来近似分布 P
时所<strong>损失的信息量</strong>。它表示的是使用分布 Q 而不是分布 P
进行编码时，所增加的额外信息量。</p>
<h4 id="特性">特性</h4>
<ol type="1">
<li><p>非负性：KL 散度总是非负的，即 <span
class="math inline">\(D_{KL}(P||Q) \geq 0\)</span>。当且仅当 <span
class="math inline">\(P=Q\)</span> 时，KL 散度为 0。</p>
<p>证明：<span class="math inline">\(-log(x)\)</span> 为凸函数，根据
Jenson 不等式：</p>
<p><span class="math display">\[
\begin{aligned}
D_{KL} &amp;= \sum_{i=1}^{n} P(x_i)\cdot \log \frac{P(x_i)}{Q(x_i)}\\
&amp;= \sum_{i=1}^{n} P(x_i)\cdot (-\log \frac{Q(x_i)}{P(x_i)})\\
&amp;\geq -\log(\sum_{i=1}^{n}P(x_i)\cdot \frac{Q(x_i)}{P(x_i)}) \\
&amp;=-\log (\sum_{i=1}^{n}Q(x_i))\\
&amp;=0
\end{aligned}
\]</span></p></li>
<li><p>非对称性：KL 散度是非对称的，<span
class="math inline">\(D_{KL}(P||Q) \neq D_{KL}(Q||P)\)</span>。</p></li>
</ol>
<blockquote>
<p>（需要高斯积分技巧）高斯分布的 KL 散度：若 <span
class="math inline">\(p \sim N(\mu_1, \sigma_1^2)\)</span>，<span
class="math inline">\(q\sim N(\mu_2,\sigma_2^2)\)</span>​，则有：</p>
<p><span class="math display">\[
\begin{aligned}
D_{KL}(p\| q) &amp;=
\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma_1}e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}}\cdot
\log(\frac{e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}}/\sigma_1}{e^{-\frac{(x-\mu_2)^2}{2\sigma_2^2}}/\sigma_2})
\\
&amp;=\log \frac{\sigma_2}{\sigma_1} +
\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2}
\end{aligned}
\]</span></p>
<p>特别的，若 <span
class="math inline">\(\sigma_1=\sigma_2=\sigma\)</span>：</p>
<p><span class="math display">\[
D_{KL}(p\|q)=\frac{1}{2\sigma^2}\|\mu_1-\mu_2\|^2
\]</span></p>
</blockquote>
<ol start="3" type="1">
<li><p>KL 散度可以写为：</p>
<p><span class="math display">\[
D_{KL}(p|| q) = H(p, q) - H(p)
\]</span></p>
<p>也就是 <strong>交叉熵 - 原分布的熵</strong></p></li>
<li><p>KL 散度还可以写为： <span class="math display">\[
D_{KL}(p|| q) = \mathbb{E}_{x\sim p(x)}\left[\log
\frac{p(x)}{q(x)}\right]
\]</span> 也就是 <strong><span class="math inline">\(x\)</span> 服从分布
<span class="math inline">\(p(x)\)</span> 下，<span
class="math inline">\(\log \frac{p(x)}{q(x)}\)</span>
的期望。</strong></p></li>
</ol>
<h3 id="重参数化方法-reparameterization">重参数化方法
Reparameterization</h3>
<p>若想从目标分布 <span class="math inline">\(\mathcal{N}(\mu,
\sigma^2)\)</span> 进行采样，可以先从标准高斯分布中采样得到一个随机变量
<span
class="math inline">\(\epsilon\)</span>，将采样的随机变量进行线性变换：</p>
<p><span class="math display">\[
z = \mu + \sigma\cdot\epsilon, \quad \epsilon \sim \mathcal{N}(0,I)
\]</span></p>
<blockquote>
<p>重参数化特性：<strong>可微性</strong>，
这种重参数化方式使得采样过程保持可微性，因为 <span
class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma\)</span> 只是通过简单的线性变换作用在 <span
class="math inline">\(\epsilon\)</span>
上。这意味着在反向传播时，梯度可以通过 <span
class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma\)</span>​ 传递，从而可以优化这些参数。</p>
</blockquote>
<p>下文中有一个记号：</p>
<p><span class="math display">\[
\mathcal{N}(x;\mu,\sigma^2)
\]</span></p>
<p>表示 <span class="math inline">\(x\)</span> 是 <span
class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span>
分布的一个抽样，利用重参数方法，可以写作：</p>
<p><span class="math display">\[
x = \mu + \sigma\cdot\epsilon, \quad \epsilon \sim \mathcal{N}(0,I)
\]</span></p>
<h3 id="高斯分布的线性组合">高斯分布的线性组合</h3>
<p>设 x 和 y 是<strong>独立</strong>的高斯分布随机变量，并且 <span
class="math inline">\(x \sim \mathcal{N}(\mu_x, \sigma_x^2)\)</span>
，<span class="math inline">\(y \sim \mathcal{N}(\mu_y,
\sigma_y^2)\)</span>，则其线性组合：</p>
<p><span class="math display">\[
\alpha x + \beta y \sim \mathcal{N}(\alpha\mu_x + \beta\mu_y,
\alpha^2\sigma_x^2+\beta^2\sigma_y^2)
\]</span></p>
<p>特别的，若 x 和 y 是<strong>独立的标准高斯分布</strong>，也就是 <span
class="math inline">\(x\sim\mathcal{N}(0,I)\)</span>，<span
class="math inline">\(y\sim\mathcal{N}(0,I)\)</span>，则其线性组合：</p>
<p><span class="math display">\[
\alpha x + \beta y \sim \mathcal{N}(0, \alpha^2+\beta^2)
\]</span></p>
<h3 id="似然函数-lf-和最大似然估计-mle">似然函数 LF 和最大似然估计
MLE</h3>
<ol type="1">
<li><p>似然函数 <code>Likelihood Function</code></p>
<p>度量在给定参数的条件下，观测数据的可能性。</p>
<p>数学描述：给定参数 <span class="math inline">\(\theta\)</span>
和一组观测数据 <span class="math inline">\(x\)</span>，似然函数 <span
class="math inline">\(L(\theta)\)</span>
定义为观测数据在这些参数下出现的概率。表示为：</p>
<p><span class="math display">\[
L(\theta;x) = p(x|\theta)
\]</span></p>
<p>对数似然函数可以表示为：</p>
<p><span class="math display">\[
\ell(\theta;x)=\log L(\theta;x) = \log p(x|\theta)
\]</span></p></li>
</ol>
<blockquote>
<p>例如设一枚硬币抛出时正面朝上的概率为 <span
class="math inline">\(p\)</span>​，做三次独立抛掷实验，发现两次向上一次向下，如何建
模这个实验，就是似然函数可以干的事情。虽然似然函数可以用概率分布来计算，但是似然函数并不等同于概率分布（比如在概率分布后加一个常数也可以作为似然函数）[From
ChatGPT 4o]：</p>
<ul>
<li><p><strong>概率密度函数</strong>用于描述在给定参数下数据出现的概率。</p></li>
<li><p><strong>似然函数</strong>用于在给定观测数据下估计参数的可能性。</p></li>
</ul>
</blockquote>
<ol start="2" type="1">
<li><p>最大似然估计 <code>Maximum Likelihood Estimation</code></p>
<p>找一组参数 <span
class="math inline">\(\theta\)</span>，使得观测数据的似然函数最大化。一般先取
<span class="math inline">\(\log\)</span>​
求最大值。这里也可以看出：<strong>似然函数并不一定是概率</strong>，也并不一定是
0 到 1 之间。</p></li>
</ol>
<h3
id="expectation-maximization-算法和置信下界-elbo">Expectation-Maximization
算法和置信下界 ELBO</h3>
<blockquote>
<p>参考 ，EM
算法用于求解<strong>不完备数据的最大似然估计</strong>，在这种情况中，直接对观测数据求最大似然估计比较困难。但是可以借用隐变量来获取似然函数的置信下界。</p>
<p>注：这里下标 <span
class="math inline">\(\mathbb{E}_{q(z|x,\theta)}\)</span> 均为 <span
class="math inline">\(\mathbb{E}_{z\sim q(z|x,\theta)}\)</span>
的省略。</p>
</blockquote>
<p>设完备数据为 <span class="math inline">\((x,z)\)</span>，其中 <span
class="math inline">\(x\)</span> 为观测数据，<span
class="math inline">\(z\)</span> 不可观测数据（潜在变量或隐变量
<code>latent variation</code>）</p>
<p>设观测数据 <span class="math inline">\(x\)</span> 的似然函数为 <span
class="math inline">\(L(\theta;x) =
p(x|\theta)\)</span>​，由联合分布的特性：</p>
<p><span class="math display">\[
\begin{aligned}
L(\theta ; x) &amp;= p(x|\theta)=\int p(x, z | \theta) dz \\
&amp;= \int \frac{q(z|x, \theta) p(x, z | \theta)}{q(z|x, \theta)} dz \\
&amp;= \mathbb{E}_{q(z|x, \theta)} \left[ \frac{p(x, z | \theta)}{q(z|x,
\theta)} \right]
\end{aligned}
\]</span></p>
<p>其中 $ p(y, z | ) $ 为完备数据的密度函数，$ q(z|x, ) $
为密度函数且满足：如果 $ q(z|x, ) = 0 $，则 $p(x, z | ) = 0
$。对数似然函数：</p>
<p><span class="math display">\[
\ell (\theta ; x) = \log \left( \mathbb{E}_{q(z|x, \theta)} \left[
\frac{p(x, z | \theta)}{q(z|x, \theta)} \right] \right) \geq
\mathbb{E}_{q(z|x, \theta)} \left[ \log \left( \frac{p(x, z |
\theta)}{q(z|x, \theta)} \right) \right])。
\]</span></p>
<p>其中应用了<span class="math inline">\(-log(x)\)</span> 为凸函数和
Jenson 不等式。根据 Jenson 不等式的取等条件，当且仅当 <span
class="math inline">\(p(x, z | \theta)/q(z|x, \theta)\)</span>​ 是一个与
<span class="math inline">\(z\)</span> 无关的常数，等号成立。</p>
<p>还有一种方式可以推导出这个结果，利用 <span class="math inline">\(\int
q(z|x,\theta) \text{d} z= 1\)</span>，可得：</p>
<p><span class="math display">\[
\begin{aligned}
\ell (\theta ; x) &amp;= \log p(x|\theta) \\
&amp;= \log p(x|\theta) \int q(z|x,\theta) \text{d}z \\
&amp;= \int q(z|x,\theta)\log p(x|\theta) \text{d}z \\
&amp;= \mathbb{E}_{q (z|x, \theta)} [\log p(x|\theta)] \\
&amp;= \mathbb{E}_{q (z|x, \theta)} \left[ \log \frac{p(x,
z|\theta)}{p(z|x,\theta)} \right] \\
&amp;= \mathbb{E}_{q (z|x, \theta)} \left[ \log \frac{p(x, z|\theta)q
(z|x, \theta)}{p(z|x,\theta)q (z|x, \theta)} \right] \\
&amp;= \mathbb{E}_{q (z|x, \theta)} \left[ \log \frac{p(x, z|\theta)}{q
(z|x, \theta)} \right] + \mathbb{E}_{q (z|x, \theta)} \left[ \log
\frac{q (z|x, \theta)}{p (z|x, \theta)} \right] \\
&amp;= \mathbb{E}_{q (z|x, \theta)} \left[ \log \frac{p(x, z|\theta)}{q
(z|x, \theta)} \right] + D_{KL}(q (z|x, \theta)||p (z|x, \theta)) \\
&amp;\geq \mathbb{E}_{q (z|x, \theta)} \left[ \log \frac{p(x,
z|\theta)}{q (z|x, \theta)} \right]
\end{aligned}
\]</span></p>
<p>当且仅当 <span class="math inline">\(q(z|x,\theta) =
p(z|x,\theta)\)</span> 时，取等号。事实上，这和上述的取等条件：当且仅当
<span class="math inline">\(p(x, z | \theta)/q(z|x, \theta)\)</span>
是一个与 <span class="math inline">\(z\)</span> 无关的常数是一致的。</p>
<blockquote>
<p>若 <span class="math inline">\(q(z|x,\theta) =
p(z|x,\theta)\)</span>，则：</p>
<p><span class="math display">\[
\frac{p(x, z | \theta)}{q(z|x, \theta)} = p(x|\theta)\cdot
\frac{p(z|x,\theta)}{q(z|x,\theta)} = p(x|\theta)
\]</span></p>
<p>是一个与 <span class="math inline">\(z\)</span>​ 无关的常数。</p>
<p>若 <span class="math inline">\(p(x, z | \theta)/q(z|x,
\theta)\)</span> 是一个与 <span class="math inline">\(z\)</span>
无关的常数：</p>
<p><span class="math display">\[
\frac{p(x, z | \theta)}{q(z|x, \theta)} = p(x|\theta)\cdot
\frac{p(z|x,\theta)}{q(z|x,\theta)}
\]</span></p>
<p>所以 <span
class="math inline">\(\frac{p(z|x,\theta)}{q(z|x,\theta)}\)</span>
必是一个与 <span class="math inline">\(z\)</span> 无关的常数，由于二者对
z 的积分均为 1，所以该常数只能为 1，也就是 <span
class="math inline">\(q(z|x,\theta) = p(z|x,\theta)\)</span></p>
</blockquote>
<p>也就是说：<strong>最大似然有一个置信下界（Evidence Lower
Bound）ELBO</strong></p>
<p>EM 算法后续的迭代过程忽略。</p>
<h3 id="联合分布和边缘分布">联合分布和边缘分布</h3>
<p>对于两个连续型随机变量 <span class="math inline">\(X\)</span> 和
<span class="math inline">\(Y\)</span> 的联合概率密度函数 <span
class="math inline">\(f_{X,Y}(x,y)\)</span>​，可以通过积分得到各自的边缘概率密度函数。</p>
<p><span class="math display">\[
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \, dy
\]</span></p>
<p><span class="math display">\[
f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \, dx
\]</span></p>
<p>条件分布也可以通过联合分布和边缘分布得到：</p>
<p><span class="math display">\[
f_{Y \mid X}(y \mid x) = \frac{f_{X,Y}(x,y)}{f_X(x)}
\]</span></p>
<h3 id="fubini-定理">Fubini 定理</h3>
<p>假设 <span class="math inline">\(A\)</span> 和 <span
class="math inline">\(B\)</span> 是完备测度空间。假设 <span
class="math inline">\(f(x,y)\)</span> 是 <span class="math inline">\(A
\times B\)</span> 上的可测函数。</p>
<p>如果 <span class="math inline">\(f(x,y)\)</span> 在 <span
class="math inline">\(A\times B\)</span> 上绝对可积，即：</p>
<p><span class="math display">\[
\int_{A \times B} |f(x, y)| \, d(x, y) &lt; \infty,
\]</span></p>
<p>那么</p>
<p><span class="math display">\[
\int_A \left( \int_B f(x, y) \, dy \right) dx = \int_B \left( \int_A
f(x, y) \, dx \right) dy = \int_{A \times B} f(x, y) \, d(x, y)
\]</span></p>
<h2 id="生成模型-generative-model">生成模型 Generative Model</h2>
<p>Generative Model
本质是通过一个已知的概率模型来拟合所给的数据样本。</p>
<p>需要通过模型得到一个带参数的分布。即如果训练数据的分布函数为 <span
class="math inline">\(p_{data}(x)\)</span> ，生成样本的分布函数为 <span
class="math inline">\(p_{model}(x)\)</span>​
，希望<strong>得到的分布</strong>和<strong>训练数据的分布</strong>尽可能相似。</p>
<h3 id="variation-autoencoder-vae">Variation AutoEncoder VAE</h3>
<p>VAE 的架构设计了两个网络：Encoder，将观察的数据 x 映射到隐变量 z
上；Decoder，从采样的 z 中解码回 x。从实践的角度，VAE 需要训练两套参数
<span class="math inline">\(\theta\)</span> 和 <span
class="math inline">\(\phi\)</span>​</p>
<p>VAE 方法使用最大似然路线和上述的 ELBO。看起来很完美，但是问题在于
ELBO
只是一个下界，离真实分布很远的下界也是下界。必须把这个下界抬的足够高才能得到一个好的效果。由于需要用深度学习网络来建模近似分布
<span
class="math inline">\(q(z|\eta)\)</span>，而深度学习的优化算法是基于反向传播算法（Back
Propagation），如果要从分布 <span
class="math inline">\(q(z|\eta)\)</span>​
中采样，采样这个操作会<strong>打断梯度的传递</strong>。</p>
<p>所以说使用最大似然路线的 VAE 方法，效果并不如
GAN。要想避免这个问题，只能使用高斯分布加重参数化的技巧来避免无法求梯度的问题。事实上，VAE
算法在实现的时候增加了多个假设，有些牵强，反过来限制了它能力的上限。</p>
<p>业界针对 VAE 的问题做了很多尝试，DM
可以算作是其中一种比较成功的尝试。</p>
<h2 id="denoising-diffusion-probabilistic-model">Denoising Diffusion
Probabilistic Model</h2>
<p>VAE 想一步到位，同时训练 Encoder 和
Decoder，能够将原始图片映射到隐变量中，又能解码回来，看起来是非常困难的。一个常见的降低问题难度的思路是能不能只训练
Encoder 或 Decoder。而 Diffusion Model 就是去掉了 Encoder
部分的学习，只关心如何学好 Decoder，即逆向过程中如何重建数据。</p>
<p>DDPM 建模：</p>
<img src="/project/2024/08/08/Denoising-Diffusion-Probabilistic-Model/1.png" class="" title="DDPM">
<h3 id="前向过程">前向过程</h3>
<h4 id="简单介绍">简单介绍</h4>
<p>输入图像 <span class="math inline">\(x_0\)</span>，经过时间 <span
class="math inline">\(T\)</span>​ 个步骤，逐步向其添加高斯噪声。</p>
<blockquote>
<p>这里的前向过程并不等同于 MLP 中的前向过程 [From ChatGPT 4o]：</p>
<ul>
<li>Denoising Deffusion Model
的前向过程是一个<strong>逐步添加噪声</strong>的过程。目标是学习在<strong>添加噪声</strong>和<strong>去除噪声</strong>之间的映射，从而能够从噪声中恢复原始图像。</li>
<li>MLP
的前向过程是通过多层计算，将输入数据逐步<strong>变换成输出数据</strong>。目标是通过学习数据的特征，进行
Classfication 或 Regression 等下游任务。</li>
</ul>
</blockquote>
<p>当 <span class="math inline">\(T\)</span>​
足够大时，得到的加噪后的图像便接近一个<strong>高斯噪声图像</strong>。DDPM
Paper 中给出的典型值为 1000。</p>
<h4 id="数学描述">数学描述</h4>
<blockquote>
<p>设在前向的每一步通过向图像 <span
class="math inline">\(x_{t-1}\)</span> 中添加高斯噪声得到 <span
class="math inline">\(x_t\)</span> 的过程称为 <span
class="math inline">\(q(x_t|x_{t-1})\)</span>​</p>
<p><strong>前向过程没有任何参数，也没有任何训练过程！</strong></p>
</blockquote>
<p>给定初始分布 <span class="math inline">\(x_0 \sim
q(x)\)</span>，当前时刻 <span class="math inline">\(t\)</span>
下，添加的高斯噪声仅仅与当前时刻的一个固定值 <span
class="math inline">\(\beta_t\)</span> （<span
class="math inline">\(\beta\)</span> 一般是个很小的值，原文中取了 <span
class="math inline">\(\beta_1= 1\times 10^{-4}\)</span> 到 <span
class="math inline">\(\beta_T = 0.02\)</span>
的等差数列）和前一个时刻的状态 <span
class="math inline">\(x_{t-1}\)</span> 而定，加入的高斯噪声均值为 <span
class="math inline">\(\mu_t = \sqrt{1-\beta_t}x_{t-1}\)</span>，方差为
<span class="math inline">\(\Sigma_t = \beta_t
I\)</span>，故前向过程为：</p>
<p><span class="math display">\[
q(x_t|x_{t-1}) = \mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1}, \beta_t I)
\]</span></p>
<p>DDPM 的前向过程可以认为是一个 Markov 过程，从输入 <span
class="math inline">\(x_0\)</span> 到 <span
class="math inline">\(x_T\)</span> 的后验概率分布可以表示为：</p>
<p><span class="math display">\[
q(x_{1:T}|x_0) = \prod _{t=1}^{T}q(x_t|x_{t-1})
\]</span></p>
<blockquote>
<p>需要注意的是：这里 <span
class="math inline">\(q(x_t|x_{t-1})\)</span>
<strong>并不是两个状态之间的转移概率</strong>！！并且其本身<strong>不是一个条件概率，是一个条件概率分布</strong>！只是一般条件概率分布写作
<span
class="math inline">\(q_{X|Y}(x|y)\)</span>。<strong>条件概率分布同样也适用
Bayes 定理。</strong></p>
<p>这两个状态之间的转移概率应该为 1，因为按照原文的话：... called the
forward process or diffusion process: <strong>is fixed to</strong> a
Markov chain that gradually adds Gaussian noise to the data
...，也就是说 <span class="math inline">\(t-1\)</span> 状态一定会转移到
<span class="math inline">\(t\)</span> 状态。</p>
<p>另外<strong>这里 <span class="math inline">\(x_{1:T}\)</span> 表示
<span class="math inline">\(x_1,x_2,\cdots,x_T\)</span>
的联合分布</strong>，下文同理。</p>
</blockquote>
<p><strong>但是这样算太麻烦了，为了得到 <span
class="math inline">\(x_T\)</span>，就得重复采样 <span
class="math inline">\(T\)</span>
次再求积。</strong>但是可以使用重参数化方法来简化这个过程：</p>
<p>对任意的时间 <span class="math inline">\(t\)</span>，为了得到 <span
class="math inline">\(t\)</span> 时刻的采样：
每一个状态首先采集一个二维标准正态分布变量 <span
class="math inline">\(\epsilon_{1:T} \sim \mathcal{N}(0,
I)\)</span>，然后通过参数 <span class="math inline">\(\beta_t\)</span>
由 <span class="math inline">\(x_{t-1}\)</span> 获得 <span
class="math inline">\(x_t\)</span>：</p>
<p><span class="math display">\[
x_t = \sqrt{1 - \beta_t} x_{t-1} + \sqrt{\beta_t} \epsilon_{t-1}
\]</span></p>
<p>按照原论文的符号表示：<span class="math inline">\(\alpha_t =
1-\beta_t\)</span>：</p>
<p><span class="math display">\[
\begin{aligned}
x_t &amp;= \sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon_{t-1}\\
&amp;=\sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_{t-1}}\epsilon_{t-2})+\sqrt{1-\alpha_t}\epsilon_{t-1}\\
&amp;=\sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{\alpha_t} \cdot
\sqrt{1 - \alpha_{t-1}} \epsilon_{t-2} + \sqrt{1 - \alpha_t}
\epsilon_{t-1}
\end{aligned}
\]</span></p>
<p>由于 <span class="math inline">\(\epsilon_{t-1}\)</span> 和 <span
class="math inline">\(\epsilon_t\)</span>​ 均服从标准高斯分布 <span
class="math inline">\(\mathcal{N}(0,
I)\)</span>，<strong>由高斯分布的线性组合</strong>，有：</p>
<p><span class="math display">\[
\begin{aligned}
\left(\sqrt{\alpha_t} \cdot \sqrt{1 -
\alpha_{t-1}}\right)^2+\left(\sqrt{1 - \alpha_t} \right)^2 = 1-\alpha_t
\alpha_{t-1}
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
x_t &amp;=\sqrt{\alpha_t \alpha_{t-1}} x_{t-2}  + \sqrt{1 - \alpha_t
\alpha_{t-1}} \bar{\epsilon}_{t-2}
\end{aligned}
\]</span></p>
<p>逐渐推导到 <span class="math inline">\(x_0\)</span>，并设 <span
class="math inline">\(\overline{\alpha_{t}} =
\alpha_t\alpha_{t-1}\cdots\alpha_1\)</span> 可得：</p>
<p><span class="math display">\[
x_t = \sqrt{\bar{\alpha_t}}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon
\]</span></p>
<p>也可以写为：</p>
<p><span class="math display">\[
x_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 -
\bar{\alpha}_t} \epsilon \right)
\]</span></p>
<p>综上，在已知初始样本分布：<span class="math inline">\(x_0 \sim
q(x)\)</span> 的情况下，为什么在时间步 <span
class="math inline">\(t\)</span> 产生一个样本，可以根据表达式：</p>
<p><span class="math display">\[
x_t \sim q(x_t|x_0) = \mathcal{N}\left( x_t; \sqrt{\bar{\alpha}_t} x_0,
(1 - \bar{\alpha}_t) \mathbf{I} \right)
\]</span></p>
<p>其中 <span class="math inline">\(\beta_t\)</span>
是一个超参数，可以<strong>预先计算所有 <span
class="math inline">\(\alpha_t\)</span> 和 <span
class="math inline">\(\overline{\alpha_t}\)</span> 以加速。</strong></p>
<h3 id="反向过程">反向过程</h3>
<h4 id="简单介绍-1">简单介绍</h4>
<p>从噪声图像 <span class="math inline">\(x_T\)</span>
开始，通过一个神经网络学习 <span class="math inline">\(x_{t-1}\)</span>
到 <span class="math inline">\(x_t\)</span>
添加的噪声，然后通过逐渐去噪的方式得到最后要生成的图像。</p>
<blockquote>
<p>注意：</p>
<ul>
<li>前向过程中的 <span class="math inline">\(x_T\)</span>
是由初始的干净图像 <span class="math inline">\(x_0\)</span> 加 <span
class="math inline">\(T\)</span> 步高斯噪声得来，当 <span
class="math inline">\(T\)</span> 足够大时，收敛到各向同性的高斯噪声</li>
<li>反向过程中的 <span class="math inline">\(x_T\)</span>
直接取随机噪声</li>
</ul>
</blockquote>
<h4 id="数学描述-1">数学描述</h4>
<blockquote>
<p>注：这里为了和原文保持一致，由于前向过程没有参数，而反向过程有参数
<span class="math inline">\(\theta\)</span>，所以将反向过程 <span
class="math inline">\(p(x|\theta)\)</span> 写作 <span
class="math inline">\(p_\theta(x)\)</span>，前向过程写作 <span
class="math inline">\(q(x)\)</span>。与数学基础的符号表示（<span
class="math inline">\(p(\theta|x)\)</span>）有出入。</p>
</blockquote>
<p>首先，当时间步长 <span class="math inline">\(T \to \infty\)</span>
时，隐变量 <span class="math inline">\(x_T\)</span>
可以认为是一个各向同性的高斯分布，反之，DDPM 的反向过程 <span
class="math inline">\(p(x_{t-1}|x_t)\)</span>
则是一个去噪过程。即先随机采样一个二位高斯噪声，然后逐步去噪，最终得到一个和真实图像分布一致的生成图像
<span class="math inline">\(x_0\)</span>。</p>
<p>然而反向过程 <span class="math inline">\(p(x_{t-1}|x_t)\)</span>
是未知的，DDPM 指出，可以使用一个神经网络来学习这个去噪过程：在时刻
<span class="math inline">\(t\)</span> 的分布 <span
class="math inline">\(x_t\)</span> 是已知的，神经网络的目的是根据 <span
class="math inline">\(x_t\)</span> 来学习（近似） <span
class="math inline">\(x_{t-1}\)</span> 的概率分布函数 <span
class="math inline">\(p_\theta(x_{t-1}|x_t)\)</span>。为了简化模型的训练难度，DDPM
假设反向去噪过程<strong>滤除的也是高斯噪声</strong>，并且只对均值和方差进行参数化。DDPM
的反向过程可以建模为：</p>
<p><span class="math display">\[
p_\theta(x_{t-1}|x_t) =
\mathcal{N}(x_{t-1};\mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t))
\]</span></p>
<p>其中 <span class="math inline">\(\theta\)</span>​ 是模型的参数。</p>
<p>同时，DDPM 的前向过程中我们假设每一个时间扩散均为 Markov
过程，那么其反向过程也可以认为是 Markov
过程，所以反向过程可以表示为：</p>
<p><span class="math display">\[
p_\theta (x_{0:T}) = p(x_T) \cdot \prod_{t=1}^{T} p_\theta (x_{t-1}|x_t)
\]</span></p>
<p>其中，<span class="math inline">\(p(x_T) =
\mathcal{N}(x_T;0,I)\)</span> 是随机采样的高斯噪声（所以这里不需要写参数
<span class="math inline">\(\theta\)</span>），<span
class="math inline">\(p_\theta (\mathbf{x}_{t-1}|\mathbf{x}_t)\)</span>
是一个均值和方差需要计算的高斯分布。</p>
<h3 id="denoising-diffusion-model-的训练和采样">Denoising Diffusion
Model 的训练和采样</h3>
<h4 id="损失函数推导">损失函数推导</h4>
<p>由 Diffusion 的反向过程，可以自然的想到，真实数据分布是 <span
class="math inline">\(x_0\sim
q(x_0)\)</span>，反向过程建模的最终结果概率分布可以写作 <span
class="math inline">\(p_\theta(x_0)\)</span>，为了衡量二者分布的相似性，在深度学习中，显然的，可以使用交叉熵作为其损失函数，并使用优化算法得到最优参数：</p>
<p><span class="math display">\[
L_{ideal}=H(p_\theta(x_0),q(x_0))=\mathbb{E}_{q(x_0)}(-\log
p_\theta(x_0))
\]</span></p>
<p>由于我们无法直接知道反向过程的建模，也就是无法直接得出 <span
class="math inline">\(p_\theta(x_0)\)</span>，但是在整个过程中，我们有隐变量
<span
class="math inline">\(x_{1:T}\)</span>，可以辅助我们获得似然函数的置信下界（ELBO），结合数学基础（注意这里由于负号的加入，不等式方向有所改变），推导如下：</p>
<p><span class="math display">\[
\begin{aligned}
-\log p_\theta(x_0) &amp;\leq -\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log
\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}\right]\\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log
\frac{q(x_{1:T}|x_0)}{p_\theta(x_{0:T})}\right] \\
\end{aligned}
\]</span></p>
<p>对该不等式左右取期望，设右边为 <span
class="math inline">\(L\)</span>，利用 Fubini 定理：</p>
<p><span class="math display">\[
\begin{aligned}
L_{ideal}
&amp;\leq\mathbb{E}_{q(x_0)}\left(\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log
\frac{q(x_{1:T}|x_0)}{p_\theta(x_{0:T})}\right]\right) \\
&amp;=\mathbb{E}_{q(x_{0:T})}\left[\log
\frac{q(x_{1:T}|x_0)}{p_\theta(x_{0:T})}\right] \\
&amp;:= L
\end{aligned}
\]</span></p>
<p>之后就是<strong>一段冗长</strong>的代入表达式，运用 Beyas
公式简化这个损失函数的过程：</p>
<blockquote>
<p>推导过程会用到一个 Bayes 公式：</p>
<p><span class="math display">\[
\begin{aligned}
q(x_{t-1} | x_t, x_0) &amp;= \frac{q(x_{t-1}, x_t, x_0)}{q(x_t, x_0)}\\
&amp;= q(x_t | x_{t-1}, x_0) \cdot \frac{q(x_{t-1}, x_0)}{q(x_t, x_0)}
\\
&amp;=q(x_t | x_{t-1}, x_0) \cdot \frac{q(x_{t-1} | x_0)}{q(x_t |
x_0)}\\
&amp;=q(x_t | x_{t-1}) \cdot \frac{q(x_{t-1} | x_0)}{q(x_t | x_0)} \quad
\end{aligned}
\]</span></p>
<p>所以：</p>
<p><span class="math display">\[
q(x_t | x_{t-1})=q(x_{t-1} | x_t, x_0)\cdot\frac{q(x_t | x_0)}{q(x_{t-1}
| x_0)}
\]</span></p>
</blockquote>
<p><span class="math display">\[
\begin{aligned}
L &amp;= \mathbb{E}_{q(x_{0:T})} \left[ \log
\frac{q(x_{1:T}|x_0)}{p_\theta(x_{0:T})} \right] \\
&amp;= \mathbb{E}_{q(x_{0:T})} \left[ \log \frac{\prod_{t=1}^T
q(x_t|x_{t-1})}{p(x_T) \prod_{t=1}^T p_\theta(x_{t-1}|x_t)} \right]
\quad 代入 \\
&amp;= \mathbb{E}_{q(x_{0:T})} \left[ -\log p(x_T) + \sum_{t=1}^T \log
\frac{q(x_t|x_{t-1})}{p_\theta(x_{t-1}|x_t)} \right] \quad \log 的性质
\\
&amp;= \mathbb{E}_{q(x_{0:T})} \left[ -\log p(x_T) + \sum_{t=2}^T \log
\frac{q(x_t|x_{t-1})}{p_\theta(x_{t-1}|x_t)} + \log
\frac{q(x_1|x_0)}{p_\theta(x_0|x_1)} \right]\\
&amp;= \mathbb{E}_{q(x_{0:T})} \left[ -\log p(x_T) + \sum_{t=2}^T \log
\left( \frac{q(x_{t-1}|x_t, x_0)}{p_\theta(x_{t-1}|x_t)}\cdot
\frac{q(x_t|x_0) }{q(x_{t-1}|x_0) }\right) + \log
\frac{q(x_1|x_0)}{p_\theta(x_0|x_1)} \right] \\
&amp;= \mathbb{E}_{q(x_{0:T})} \left[ -\log p(x_T) + \sum_{t=2}^T \log
\frac{q(x_{t-1}|x_t, x_0)}{p_\theta(x_{t-1}|x_t)} + \sum_{t=2}^T \log
\frac{q(x_t|x_0) }{q(x_{t-1}|x_0) } + \log
\frac{q(x_1|x_0)}{p_\theta(x_0|x_1)} \right] \\
&amp;= \mathbb{E}_{q(x_{0:T})} \left[  -\log p(x_T) + \sum_{t=2}^T \log
\frac{q(x_{t-1}|x_t, x_0)}{p_\theta(x_{t-1}|x_t)} + \log\frac{q(x_T|x_0)
}{q(x_1|x_0) } + \log \frac{q(x_1|x_0)}{p_\theta(x_0|x_1)} \right] \\
&amp;= \mathbb{E}_{q(x_{0:T})} \left[ \log \frac{ q(x_T|x_0)}{p(x_T)} +
\sum_{t=2}^T \log \frac{q(x_{t-1}|x_t, x_0)}{p_\theta(x_{t-1}|x_t)} -
\log p_\theta(x_0|x_1) \right] \\
\end{aligned}
\]</span></p>
<p>运用熵、KL 散度、联合分布与边缘分布的关系：</p>
<ol type="1">
<li>第一项：</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_{q(x_{0:T})} \left[ \log\frac{ q(x_T|x_0)}{p(x_T)}\right]
&amp;=\int\text{d}x_{0:T}~ q(x_{0:T})\cdot \log\frac{
q(x_T|x_0)}{p(x_T)} \\
&amp;=\int\log\frac{ q(x_T|x_0)}{p(x_T)}\text{d}x_0\text{d}x_T \int
q(x_{0:T}) \text{d}_{x_{1:T-1}}\\
&amp;=\int q(x_T)\cdot \log\frac{
q(x_T|x_0)}{p(x_T)}\text{d}x_0\text{d}x_T\\
&amp;=\int q(x_0)\text{d}x_0\int q(x_T|x_0)\cdot \log\frac{
q(x_T|x_0)}{p(x_T)}\text{d}x_T\\
&amp;=\mathbb{E}_{q(x_0)}\left[ D_{KL}(q(x_T|x_0)\|p(x_T))\right]
\end{aligned}
\]</span></p>
<ol start="2" type="1">
<li>第二项同理：</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_{q(x_{0:T})} \sum_{t=2}^T \log \frac{q(x_{t-1}|x_t,
x_0)}{p_\theta(x_{t-1}|x_t)} &amp;= \int\text{d}x_{0:T}~
q(x_{0:T})\cdot\sum_{t=2}^T \log \frac{q(x_{t-1}|x_t,
x_0)}{p_\theta(x_{t-1}|x_t)} \\
&amp;=\sum_{t=2}^T \int\log \frac{q(x_{t-1}|x_t,
x_0)}{p_\theta(x_{t-1}|x_t)}\text{d} x_0\text{d} x_{t-1}\text{d}x_t\int
q(x_{0:T})\text{d}x_{0:T/(0,t-1,t)}\\
&amp;=\sum_{t=2}^T\int q(x_0,x_{t-1},x_t) \log \frac{q(x_{t-1}|x_t,
x_0)}{p_\theta(x_{t-1}|x_t)}\text{d} x_0\text{d} x_{t-1}\text{d}x_t\\
&amp;=\sum_{t=2}^T\int q(x_0,x_t)\text{d} x_0\text{d}x_t\int
q(x_{t-1}|x_0,x_t) \log \frac{q(x_{t-1}|x_t,
x_0)}{p_\theta(x_{t-1}|x_t)}\text{d} x_{t-1}\\
&amp;=\sum_{t=2}^T\mathbb{E}_{q(x_0,x_T)}\left[D_{KL}(q(x_{t-1}|x_t,
x_0)\|p_\theta(x_{t-1}|x_t))\right]
\end{aligned}
\]</span></p>
<ol start="3" type="1">
<li>第三项也同理：</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
-\mathbb{E}_{q(x_{0:T})} \left[\log p_\theta(x_1|x_0)\right] &amp;= -
\int\text{d}x_{0:T}~ q(x_{0:T})\cdot \left[\log p_\theta(x_1|x_0)\right]
\\
&amp;=-\int \log p_\theta(x_1|x_0)\text{d}x_0 \text{d}x_1 \int
q(x_{0:T})\text{d}x_{0:T/(0,1)} \\
&amp;=-\int q(x_0,x_1) \log p_\theta(x_1|x_0)\text{d}x_0 \text{d}x_1\\
&amp;=-\mathbb{E}_{q(x_0,x_1)} \left[\log p_\theta(x_1|x_0)\right]
\end{aligned}
\]</span></p>
<p>所以最终的 Loss 可以写成：</p>
<p><span class="math display">\[
\begin{aligned}
L &amp;= \mathbb{E}_{q(x_0)}\left[ D_{KL}(q(x_T|x_0)\|p(x_T))\right] \\
&amp;+ \sum_{t=2}^T \mathbb{E}_{q(x_0,x_T)} \left[ D_{KL}(q(x_{t-1}|x_t,
x_0) \| p_\theta(x_{t-1}|x_t)) \right] -\mathbb{E}_{q(x_0,x_1)}
\left[\log p_\theta(x_1|x_0)\right]
\end{aligned}
\]</span></p>
<p>也可以写为：</p>
<p><span class="math display">\[
\begin{aligned}
&amp;L = L_T+L_{T-1}+\cdots+L_0 \\
&amp;L_T = \mathbb{E}_{q(x_0)}\left[ D_{KL}(q(x_T|x_0)\|p(x_T))\right]
\\
&amp;L_{t-1} = \mathbb{E}_{q(x_0,x_T)} \left[ D_{KL}(q(x_{t-1}|x_t, x_0)
\| p_\theta(x_{t-1}|x_t)) \right]; \quad 1 \leq t-1 \leq T - 1 \\
&amp;L_0 = -\mathbb{E}_{q(x_0,x_1)} \left[\log p_\theta(x_1|x_0)\right]
\end{aligned}
\]</span></p>
<p>这里：</p>
<ul>
<li><span class="math inline">\(L_T\)</span>
可以看作<strong>最后的噪声输入</strong>和标准的<strong>高斯先验</strong>的接近程度，因为这一部分没有可以训练的参数，我们可以将它视作常数。</li>
<li><span class="math inline">\(L_{t-1}\)</span> 可以看作真实后验分布
<span class="math inline">\(q(x_{t-1}|x_t,x_0)\)</span> 与预测分布 <span
class="math inline">\(p_\theta(x_{t-1}|x_t)\)</span>​ 的接近程度，DDPM
的目标也正是保证<strong>真实的去噪过程</strong>和<strong>模型预测的去噪过程</strong>尽可能一致。</li>
<li><span class="math inline">\(L_0\)</span>
可以看作图像重建（reconstruction）损失。</li>
</ul>
<h4 id="l_t-1-项讨论"><span class="math inline">\(L_{t-1}\)</span>
项讨论</h4>
<ol type="1">
<li><p><span class="math inline">\(p_\theta(x_{t-1}|x_t)\)</span></p>
<p>反向过程中已经介绍了，DDPM 假设反向去噪也是去除高斯噪声，所以：</p>
<p><span class="math display">\[
p_\theta(x_{t-1}|x_t) =
\mathcal{N}(x_{t-1};\mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t))
\]</span></p></li>
<li><p><span class="math inline">\(q(x_{t-1}|x_t, x_0)\)</span>​</p>
<p>为了求该分布，需要再次使用上述的 Bayes 定理：</p>
<blockquote>
<p><span class="math display">\[
\begin{aligned}
q(x_{t-1} | x_t, x_0) &amp;= \frac{q(x_{t-1}, x_t, x_0)}{q(x_t, x_0)}\\
&amp;= q(x_t | x_{t-1}, x_0) \cdot \frac{q(x_{t-1}, x_0)}{q(x_t, x_0)}
\\
&amp;=q(x_t | x_{t-1}, x_0) \cdot \frac{q(x_{t-1} | x_0)}{q(x_t |
x_0)}\\
&amp;=q(x_t | x_{t-1}) \cdot \frac{q(x_{t-1} | x_0)}{q(x_t | x_0)} \quad
\end{aligned}
\]</span></p>
</blockquote>
<p>由前向过程可知：</p>
<p><span class="math display">\[
\begin{aligned}
q(x_t|x_{t-1}) &amp;= \mathcal{N}(x_t;\sqrt{\alpha_t}x_{t-1},
(1-\alpha_t) I)\\
q(x_{t-1}|x_0) &amp;=\mathcal{N}\left( x_{t-1};
\sqrt{\bar{\alpha}_{t-1}} x_0, (1 - \bar{\alpha}_{t-1}) \mathbf{I}
\right) \\
q(x_{t}|x_0) &amp;=\mathcal{N}\left( x_{t}; \sqrt{\bar{\alpha}_t} x_0,
(1 - \bar{\alpha}_t) \mathbf{I} \right)
\end{aligned}
\]</span></p>
<p>这样代入公式，一通计算，具体计算可以查看论文 <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2208.11970">Understanding Diffusion Models:
A Unified Perspective</a> 公式 [71] 到公式
[84]。总之，它的计算结果表明：</p>
<p><span class="math display">\[
q(x_{t-1} | x_t, x_0) =
\mathcal{N}(x_{t-1},\tilde{\mu}_t(x_t,x_0),\tilde{\beta}_t I)
\]</span></p>
<p>其中：</p>
<p><span class="math display">\[
\tilde{\mu}_t (x_t, x_0) = \frac{\sqrt{\alpha_t} (1 -
\overline{\alpha_{t-1}}) x_t + \sqrt{\overline{\alpha_{t-1}}}
(1-\alpha_t) x_0}{1 - \overline{\alpha_t}}
\]</span></p>
<p><span class="math display">\[
\tilde{\beta}_t=\frac{(1 - \alpha_t)(1 - \overline{\alpha_{t-1}})}{1 -
\overline{\alpha_t}}=\frac{1 - \overline{\alpha_{t-1}}}{1 -
\overline{\alpha_t}}\beta_t
\]</span></p>
<p>发现<strong>均值是与变量相关的，而方差无关</strong>，所以可以假设所预测的分布也可以表示为：</p>
<p><span class="math display">\[
p_\theta(x_{t-1}|x_t) =
\mathcal{N}(x_{t-1};\mu_{\theta}(x_t,t),\sigma_t^2\mathbf{I})
\]</span></p>
<p>其中 <span
class="math inline">\(\sigma_t^2=\tilde{\beta}_t\)</span>，那么根据数学基础中两个高斯分布的
KL 散度，可以得到：</p>
<p><span class="math display">\[
L_{t-1}=\mathbb{E}_{q(x_0,x_T)}\left[\frac{1}{2\sigma_t^2}\|\tilde{\mu}_t
(x_t, x_0)-\mu_{\theta}(x_t,t)\|\right]
\]</span></p></li>
<li><p><span class="math inline">\(L_{t-1}\)</span></p>
<blockquote>
<p>第一项</p>
</blockquote>
<p>可以看到，第一项包含了 <span class="math inline">\(x_t\)</span> 和
<span class="math inline">\(x_0\)</span>，而第二项包含 <span
class="math inline">\(x_t\)</span> 和 <span
class="math inline">\(t\)</span>​，并不对称，但是我们有：</p>
<p><span class="math display">\[
x_t(x_0,\epsilon) = \sqrt{\overline{\alpha_t}}x_0 +
\sqrt{1-\overline{\alpha_t}}\epsilon\\
x_0 = \frac{1}{\sqrt{\overline{\alpha}_t}} \left( x_t - \sqrt{1 -
\overline{\alpha}_t} \epsilon_0 \right)
\]</span></p>
<p>代入 <span class="math inline">\(\tilde{\mu}_t (x_t, x_0)\)</span>
的表达式，一通计算可得：</p>
<p><span class="math display">\[
\tilde{\mu}_t (x_t, x_0) = \frac{1}{\sqrt{\alpha_t}} \left(x_t - \frac{1
- \alpha_t}{\sqrt{1 - \overline{\alpha_t}}}\epsilon\right)
\]</span></p>
<p>所以 <span class="math inline">\(L_{t-1}\)</span> 可以写为：</p>
<p><span class="math display">\[
L_{t-1}=\mathbb{E}_{q(x_0,x_T)}\left[\frac{1}{2\sigma_t^2}\left\|\frac{1}{\sqrt{\alpha_t}}
\left(x_t - \frac{1 - \alpha_t}{\sqrt{1 -
\overline{\alpha_t}}}\epsilon\right)
-\mu_{\theta}(x_t,t)\right\|^2\right]
\]</span></p>
<blockquote>
<p>第二项是按照第一项的格式进行假设的</p>
</blockquote>
<p>（来自 DDMP）上述公式说明 <span
class="math inline">\(\mu_\theta\)</span> 在给定 <span
class="math inline">\(x_t\)</span> 时必须预测 <span
class="math inline">\(\frac{1}{\sqrt{\alpha_t}} \left(x_t - \frac{1 -
\alpha_t}{\sqrt{1 - \overline{\alpha_t}}}\epsilon\right)\)</span>。因为
<span class="math inline">\(x_t\)</span>​
可以作为模型输入，我们可以选择如下参数化：</p>
<p><span class="math display">\[
\mu_\theta (x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left(x_t - \frac{1 -
\alpha_t}{\sqrt{1 - \overline{\alpha_t}}}\epsilon_\theta(x_t,t)\right)
\]</span></p>
<p>联立上述公式，<span class="math inline">\(L_{t-1}\)</span>
就可以被写为：</p>
<p><span class="math display">\[
L_{t-1}=\mathbb{E}_{q(x_0,x_T)}\left[\frac{(1-\alpha_t)^2}{2\sigma_t^2\alpha_t(1-\overline{\alpha_t})}\left\|\epsilon-\epsilon_\theta(x_t,t)\right\|^2\right]
\]</span></p>
<p>由于训练时 <span class="math inline">\(x_0\)</span> 和每轮的 <span
class="math inline">\(\epsilon\)</span>
才是输入，所以训练时损失（下标和值）不能出现 <span
class="math inline">\(x_t\)</span>，<span
class="math inline">\(L_{t-1}\)</span> 要改写为：</p>
<p><span class="math display">\[
L_{t-1}=\mathbb{E}_{x_0,epsilon}\left[\frac{(1-\alpha_t)^2}{2\sigma_t^2\alpha_t(1-\overline{\alpha_t})}\left\|\epsilon-\epsilon_\theta(\sqrt{\overline{\alpha_t}}x_0
+ \sqrt{1-\overline{\alpha_t}}\epsilon,t)\right\|^2\right]
\]</span></p>
<blockquote>
<p>至此，我们将之前预测 <span class="math inline">\(\tilde
\mu_t\)</span> 的过程变为了预测 <span
class="math inline">\(\epsilon\)</span> 的过程，在 DDPM
论文中提到：实际上无论预测谁都可以，预测 <span
class="math inline">\(\epsilon\)</span>
只是一种参数化的形式（事实上甚至还可以预测 <span
class="math inline">\(x_0\)</span>，只不过经过 DDPM
的验证，这种方式在实验的早期就会导致样本质量变差）。通过预测 <span
class="math inline">\(\epsilon\)</span> 和 <span
class="math inline">\(\tilde \mu_t\)</span> 的消融试验中，验证了预测
<span class="math inline">\(\epsilon\)</span> 的有效性。</p>
</blockquote>
<p>而采样时，<span class="math inline">\(x_t\)</span> 和采样 <span
class="math inline">\(z\)</span> 是输入，所以采样时不能出现 <span
class="math inline">\(x_0\)</span>，采样的过程为：</p>
<p><span class="math display">\[
p_\theta(x_{t-1}|x_t) =
\mathcal{N}(x_{t-1};\mu_{\theta}(x_t,t),\sigma_t^2\mathbf{I})
\]</span></p>
<p><span class="math display">\[
x_{t-1}=\frac{1}{\sqrt{\alpha_t}} \left(x_t - \frac{1 -
\alpha_t}{\sqrt{1 -
\overline{\alpha_t}}}\epsilon_\theta(x_t,t)\right)+\sigma_tz\quad
z\sim\mathcal{N}(0,I)~\text{if~t&gt;1,else z=0}
\]</span></p></li>
</ol>
<h4 id="l_0-项讨论"><span class="math inline">\(L_0\)</span> 项讨论</h4>
<blockquote>
<p>DDPM 原论文指出从 <span class="math inline">\(x_0\)</span> 到 <span
class="math inline">\(x_1\)</span> 应该是一个离散化过程，因为图像 RGB
值都是离散化的。DDPM 针对 <span
class="math inline">\(p_\theta(x_1|x_0)\)</span>
构建了一个离散化的分段积分累乘，有点类似基于分类目标的自回归（auto-regressive）学习。</p>
</blockquote>
<p>（DDPM Explanation）类比在普通 VAE 中的
ELMO，这一项可以使用蒙特卡洛估计来近似和优化。</p>
<p>可以看到，<strong>在采样的最后一步，使用了无噪声的采样</strong>，也就是：
<span class="math display">\[
x_0=\frac{1}{\sqrt{\alpha_1}} \left(x_1 - \frac{1 - \alpha_1}{\sqrt{1 -
\overline{\alpha_1}}}\epsilon_\theta(x_1,1)\right)
\]</span></p>
<h4 id="训练和采样">训练和采样</h4>
<p>由上述的讨论，我们已经得到了训练的优化目标（即损失函数），以及采样时的反向建模，所以，训练和采样的流程也是非常显然了：</p>
<ol type="1">
<li><p>训练（Training）</p>
<img src="/project/2024/08/08/Denoising-Diffusion-Probabilistic-Model/2.png" class="" title="Training"></li>
<li><p>采样（Sampling）</p>
<img src="/project/2024/08/08/Denoising-Diffusion-Probabilistic-Model/3.png" class="" title="Sampling"></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/project/tags/Diffusion/" rel="tag"># Diffusion</a>
              <a href="/project/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/project/2024/08/03/My-Curriculum-Vitae/" rel="prev" title="我的个人简历">
                  <i class="fa fa-angle-left"></i> 我的个人简历
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Daoyu Wang</span>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/Melmaphother" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/project/js/comments.js"></script><script src="/project/js/utils.js"></script><script src="/project/js/motion.js"></script><script src="/project/js/sidebar.js"></script><script src="/project/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/project/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/project/js/third-party/math/mathjax.js"></script>



</body>
</html>
